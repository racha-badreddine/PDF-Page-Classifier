{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlEBqz2yTJZk"
      },
      "source": [
        "# **IMPORT MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhPW1YWJTJZl",
        "outputId": "d24bad11-1b33-42ea-b62f-08140bfb322a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.5.1 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from keras.models import load_model\n",
        "import pickle\n",
        "\n",
        "# Define the paths\n",
        "model_path = 'model2.keras'\n",
        "tokenizer_path = 'tokenizer.pickle'\n",
        "label_encoder_path = 'label_encoder.pickle'\n",
        "\n",
        "model = load_model('model2.keras')\n",
        "\n",
        "# Load the tokenizer\n",
        "with open(tokenizer_path, 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "# Load the label encoder\n",
        "with open(label_encoder_path, 'rb') as handle:\n",
        "    label_encoder = pickle.load(handle)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6l7RVOnTJZn"
      },
      "source": [
        "# **PDF INPUT**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiS8b7PeVEVa",
        "outputId": "5e494001-97a3-4772-b30d-47a02c4e0a6b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting PyMuPDFb==1.24.9 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.24.9 PyMuPDFb-1.24.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJyyAnx6TJZn",
        "outputId": "6f6fe208-9a9e-4044-cdde-35a1c8caf6cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 251ms/step\n",
            "Toplam sayfa sayısı: 100\n",
            "[5, 94, 95, 96, 97, 98, 99, 100]\n"
          ]
        }
      ],
      "source": [
        "import fitz\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "\n",
        "PATH_TO_PDF = '515729.pdf'\n",
        "\n",
        "def remove_non_characters(text):\n",
        "    # regular expression pattern\n",
        "    pattern = r'[^a-zA-ZçÇğĞıİöÖşŞüÜ\\s]'\n",
        "    cleaned_text = re.sub(pattern, '', text)\n",
        "    return cleaned_text\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    pdf_document = fitz.open(pdf_path)\n",
        "\n",
        "    # List for the PDF  Text\n",
        "    text_list = []\n",
        "\n",
        "    references_index = 0\n",
        "    search_words = [\"kaynaklar\", \"literatür listesi\", \"kaynaklar dizini\", \"kaynakça\", \"referanslar\", \"bibliyografya\"]\n",
        "\n",
        "    for page_num in range(len(pdf_document)):\n",
        "        page = pdf_document.load_page(page_num)\n",
        "        page_text = page.get_text()\n",
        "        text_list.append(page_text)\n",
        "\n",
        "    # ALgorithm to find References Page\n",
        "    for page_num in range(len(pdf_document)-1, 0, -1):\n",
        "      page = pdf_document.load_page(page_num)\n",
        "      page_text = page.get_text()\n",
        "\n",
        "      lines = page_text.split('\\n')\n",
        "      lines = [line for line in lines if line.strip()]\n",
        "      for line in range(min(len(lines),5)):\n",
        "          if remove_non_characters(lines[line]).strip().lower() in search_words:\n",
        "              references_index = page_num+1\n",
        "              pdf_document.close()\n",
        "              return text_list, references_index\n",
        "\n",
        "    pdf_document.close()\n",
        "    return text_list, references_index\n",
        "\n",
        "new_texts, references_index = extract_text_from_pdf(PATH_TO_PDF)\n",
        "new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
        "new_padded_sequences = pad_sequences(new_sequences, maxlen=200, padding='post', truncating='post')\n",
        "\n",
        "predictions = tf.nn.sigmoid(model.predict(new_padded_sequences)).numpy()\n",
        "predicted_labels = (predictions > 0.5).astype(int)\n",
        "predicted_labels = label_encoder.inverse_transform(predicted_labels.flatten())\n",
        "\n",
        "output_list = []\n",
        "\n",
        "# Calculating length\n",
        "length = len(predicted_labels)\n",
        "\n",
        "# If not found make all pages after last 1 as 1\n",
        "if references_index == 0 or references_index < int(length * 0.5):\n",
        "    #update ekler(Appendix)\n",
        "    i = len(predicted_labels) - 1\n",
        "    while(predicted_labels[i] != 1):\n",
        "      predicted_labels[i] = 1\n",
        "      i -= 1\n",
        "else:\n",
        "    # Found\n",
        "    end = references_index - 1\n",
        "    found = True\n",
        "    for i in range(end, len(predicted_labels)):\n",
        "        predicted_labels[i] = 1\n",
        "\n",
        "for i in range(len(predicted_labels)):\n",
        "  if predicted_labels[i] == 1:\n",
        "    output_list.append(i+1)\n",
        "\n",
        "print(f\"Toplam sayfa sayısı: {len(predicted_labels)}\")\n",
        "print(output_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ytM_sjdgVAKG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}